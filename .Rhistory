}
}
# vector position of dominated allocations
dominated[i] = indicator
}
# return only non dominated allocations
return(combined[dominated == 0,])
}
# difference of the doubly robust scores for female and male (welfare improvement)
# (multiply with policy if pi = 1; otherwise only baseline welfare)
Wfem = S/mean(S) * (((D/ps)*(Y-m1)+m1) - ((1-D)/(1-ps)*(Y-m0)+m0))
Wmale = (1-S)/mean(1-S) * (((D/ps)*(Y-m1)+m1) - ((1-D)/(1-ps)*(Y-m0)+m0))
# baseline effect, i.e. doubly robust score of untreated individuals
# (multiply with S=s/mean(S=s) )
baseline_fem = S/mean(S) * ((1-D)/(1-ps)*(Y-m0)+m0)
baseline_male = (1-S)/mean(1-S) * ((1-D)/(1-ps)*(Y-m0)+m0)
# covariates as matrix; dimensions of the matrix
X = as.matrix(df_sample %>% select(C3, X1_D, X2_D, X3_D, X4_D, X5_D))
n = nrow(X)
p = ncol(X)+1
# data frame with the covariates for female / male
df_fem = as.matrix(cbind(1,1,X))
df_male = as.matrix(cbind(1,0,X))
# load the results of the frontier computations, store the betas
load("./results/frontier_plot.RData")
beta_P1_fem = frontier_P1_fem[, (n+1):(n+p+1)] # p+1 due to added S
beta_P1_male = frontier_P1_male[, (n+1):(n+p+1)] # p+1 due to added S
beta_P2_fem = frontier_P2_fem[, (n+1):(n+p)]
beta_P2_male = frontier_P2_male[, (n+1):(n+p)]
# Compute the welfare
# Policy function class 1
# with female >= male
welfare_fem1 = apply(beta_P1_fem, 1, function(x) mean(sapply(df_fem%*%x, function(y) ifelse(y >= 0, 1, 0))*Wfem + baseline_fem))
welfare_male1 = apply(beta_P1_fem, 1, function(x) mean(sapply(df_male%*%x, function(y) ifelse(y >= 0, 1, 0))*Wmale + baseline_male))
welfare_fem_function = cbind(welfare_fem1, welfare_male1)
# with female <= male
welfare_fem2 = apply(beta_P1_male, 1, function(x) mean(sapply(df_fem%*%x, function(y) ifelse(y >= 0, 1, 0))*Wfem + baseline_fem))
welfare_male2 = apply(beta_P1_male, 1, function(x) mean(sapply(df_male%*%x, function(y) ifelse(y >= 0, 1, 0))*Wmale + baseline_male))
welfare_male_function = cbind(welfare_fem2, welfare_male2)
# compute pareto frontier
welfare1 = pareto_dominance(welfare_fem_function, welfare_male_function)
# add minimum values for nicer plot
welfare1 = rbind(c(0.4, max(welfare1[,2])), welfare1, c(max(welfare1[,1]), 0.45))
# Policy function class 2
# with female >= male
welfare_fem1 = apply(beta_P2_fem, 1, function(x) mean(sapply(df_fem[,-2]%*%x, function(y) ifelse(y >= 0, 1, 0))*Wfem + baseline_fem))
welfare_male1 = apply(beta_P2_fem, 1, function(x) mean(sapply(df_male[,-2]%*%x, function(y) ifelse(y >= 0, 1, 0))*Wmale + baseline_male))
welfare_fem_function = cbind(welfare_fem1, welfare_male1)
# with female <= male
welfare_fem2 = apply(beta_P2_male, 1, function(x) mean(sapply(df_fem[,-2]%*%x, function(y) ifelse(y >= 0, 1, 0))*Wfem + baseline_fem))
welfare_male2 = apply(beta_P2_male, 1, function(x) mean(sapply(df_male[,-2]%*%x, function(y) ifelse(y >= 0, 1, 0))*Wmale + baseline_male))
welfare_male_function = cbind(welfare_fem2, welfare_male2)
# compute pareto frontier
welfare2 = pareto_dominance(welfare_fem_function, welfare_male_function)
# add minimum values for nicer plot
welfare2 = rbind(c(0.4, max(welfare2[,2])), welfare2, c(max(welfare2[,1]), 0.45))
# Policy function class 3
# only female >= male
welfare3 = cbind(welfare_fem1, welfare_male1)
# add minimum values for nicer plot
welfare3 = rbind(c(0.4, max(welfare3[,2])), welfare3, c(max(welfare3[,1]), 0.45))
# name the allocations after the policy function class
policy_class = c(rep("Class 1", dim(welfare1)[1]), rep("Class 2", dim(welfare2)[1]), rep("Class 3", dim(welfare3)[1]))
# create tibble
df_pareto = as.data.frame(cbind(rbind(welfare1, welfare2, welfare3), policy_class))
# update column names
names(df_pareto) = c('Wfemale', 'Wmale', 'Class')
# coerce columns from character to numeric
df_pareto = transform(df_pareto, Wfemale = as.numeric(Wfemale), Wmale = as.numeric(Wmale))
# create plot
ggplot(data = df_pareto, aes(x = Wfemale, y = Wmale)) +
geom_line(aes(color = Class), linetype = 1, linewidth = 0.7, show.legend = F) +
geom_ribbon(aes(ymin = min(Wmale), ymax = Wmale, fill = Class)) +
scale_color_manual(values=c("#69b3a2", "#404080", "darkgrey")) +
scale_fill_manual(values = alpha(c("#69b3a2", "#404080", "darkgrey"), 0.4),
labels = c("Policy Class 1", "Policy Class 2", "Policy Class 3"),
name = "Policy Function Class") +
xlab("Welfare female individuals") +
ylab("Welfare male individuals") +
theme_hc() +
theme(text = element_text(family = "Times New Roman"),
axis.title = element_text(size = 16),
axis.text = element_text(size = 12),
legend.title = element_text(size = 16),
legend.text = element_text(size = 12))
# load libraries
library(tidyverse)
library(data.table)
library(mltools)
library(caret)
#### Data preparation
data = read_csv("./data/synthetic_data.csv")
# prepare data
data = data %>% mutate(S = ifelse(data$C2 == 2, 1, 0), # recode S (0,1)
D = Z,                          # treatment vector D
Y = 1 / (1+exp(-data$Y)))       # normalise Y (0,1)
# one hot encoding for categorical data
data$C1 = as_factor(data$C1)
data$XC = as_factor(data$XC)
data = one_hot(as.data.table(data))
# function for the propensity score estimation
# df: data frame for the estimation
# D: the treatment vector
# K: the number of folds for cross-fitting
estimate_ps = function(df, D, K = 5) {
# load required libraries
library(caret)
library(glmnet)
# create the folds for the cross-fitting
set.seed(777)
folds = createFolds(D, k = K)
# storage for the predictions
ps = rep(0, nrow(df))
# initiate for loop
for (i in 1:K){
# split data in training and test set
test_indices = unlist(folds[i])
train_indices = unlist(folds[-i])
# remove D as the first column of df
x_train = as.matrix(df[train_indices, -1])
y_train = D[train_indices]
x_test = as.matrix(df[test_indices, -1])
# cross validation for lambda
ps_reg = cv.glmnet(x_train, as.factor(y_train), family = "binomial")
# make predictions using the test set and optimal lambda
ps[test_indices] = predict(ps_reg, s = ps_reg$lambda.min, newx = x_test, type = "response")
}
# return the ps
return(ps)
}
# prepare the df for the propensity score estimation
df_ps = data %>% select(D, S, everything(), -Y, -C2, -schoolid, -Z)
# estimate and add the propensity score to df_ps
df_ps$ps = estimate_ps(df = df_ps, D = df_ps$D, K = 5)
library(caret)
library(glmnet)
library(caret)
install.packages("caret")
library
library(caret)
# function for the propensity score estimation
# df: data frame for the estimation
# D: the treatment vector
# K: the number of folds for cross-fitting
estimate_ps = function(df, D, K = 5) {
# load required libraries
library(caret)
library(glmnet)
# create the folds for the cross-fitting
set.seed(777)
folds = createFolds(D, k = K)
# storage for the predictions
ps = rep(0, nrow(df))
# initiate for loop
for (i in 1:K){
# split data in training and test set
test_indices = unlist(folds[i])
train_indices = unlist(folds[-i])
# remove D as the first column of df
x_train = as.matrix(df[train_indices, -1])
y_train = D[train_indices]
x_test = as.matrix(df[test_indices, -1])
# cross validation for lambda
ps_reg = cv.glmnet(x_train, as.factor(y_train), family = "binomial")
# make predictions using the test set and optimal lambda
ps[test_indices] = predict(ps_reg, s = ps_reg$lambda.min, newx = x_test, type = "response")
}
# return the ps
return(ps)
}
# prepare the df for the propensity score estimation
df_ps = data %>% select(D, S, everything(), -Y, -C2, -schoolid, -Z)
# load libraries
library(tidyverse)
library(data.table)
library(mltools)
library(caret)
library(glmnet)
library(extrafont)
font_import()
data = read_csv("./data/synthetic_data.csv")
# prepare data
data = data %>% mutate(S = ifelse(data$C2 == 2, 1, 0), # recode S (0,1)
D = Z,                          # treatment vector D
Y = 1 / (1+exp(-data$Y)))       # normalise Y (0,1)
# one hot encoding for categorical data
data$C1 = as_factor(data$C1)
data$XC = as_factor(data$XC)
data = one_hot(as.data.table(data))
# function for the propensity score estimation
# df: data frame for the estimation
# D: the treatment vector
# K: the number of folds for cross-fitting
estimate_ps = function(df, D, K = 5) {
# load required libraries
library(caret)
library(glmnet)
# create the folds for the cross-fitting
set.seed(777)
folds = createFolds(D, k = K)
# storage for the predictions
ps = rep(0, nrow(df))
# initiate for loop
for (i in 1:K){
# split data in training and test set
test_indices = unlist(folds[i])
train_indices = unlist(folds[-i])
# remove D as the first column of df
x_train = as.matrix(df[train_indices, -1])
y_train = D[train_indices]
x_test = as.matrix(df[test_indices, -1])
# cross validation for lambda
ps_reg = cv.glmnet(x_train, as.factor(y_train), family = "binomial")
# make predictions using the test set and optimal lambda
ps[test_indices] = predict(ps_reg, s = ps_reg$lambda.min, newx = x_test, type = "response")
}
# return the ps
return(ps)
}
# prepare the df for the propensity score estimation
df_ps = data %>% select(D, S, everything(), -Y, -C2, -schoolid, -Z)
# estimate and add the propensity score to df_ps
df_ps$ps = estimate_ps(df = df_ps, D = df_ps$D, K = 5)
library(caret)
# load libraries
library(tidyverse)
library(data.table)
library(mltools)
library(caret)
library(glmnet)
library(extrafont)
font_import()
#### Data preparation
data = read_csv("./data/synthetic_data.csv")
# prepare data
data = data %>% mutate(S = ifelse(data$C2 == 2, 1, 0), # recode S (0,1)
D = Z,                          # treatment vector D
Y = 1 / (1+exp(-data$Y)))       # normalise Y (0,1)
# one hot encoding for categorical data
data$C1 = as_factor(data$C1)
data$XC = as_factor(data$XC)
data = one_hot(as.data.table(data))
#### Propensity score estimation
# function for the propensity score estimation
# df: data frame for the estimation
# D: the treatment vector
# K: the number of folds for cross-fitting
estimate_ps = function(df, D, K = 5) {
# load required libraries
library(caret)
library(glmnet)
# create the folds for the cross-fitting
set.seed(777)
folds = createFolds(D, k = K)
# storage for the predictions
ps = rep(0, nrow(df))
# initiate for loop
for (i in 1:K){
# split data in training and test set
test_indices = unlist(folds[i])
train_indices = unlist(folds[-i])
# remove D as the first column of df
x_train = as.matrix(df[train_indices, -1])
y_train = D[train_indices]
x_test = as.matrix(df[test_indices, -1])
# cross validation for lambda
ps_reg = cv.glmnet(x_train, as.factor(y_train), family = "binomial")
# make predictions using the test set and optimal lambda
ps[test_indices] = predict(ps_reg, s = ps_reg$lambda.min, newx = x_test, type = "response")
}
# return the ps
return(ps)
}
# prepare the df for the propensity score estimation
df_ps = data %>% select(D, S, everything(), -Y, -C2, -schoolid, -Z)
# estimate and add the propensity score to df_ps
df_ps$ps = estimate_ps(df = df_ps, D = df_ps$D, K = 5)
# plot to check overlap between treated and untreated (Positivity Check)
ggplot(df_ps) +
geom_histogram(aes(x = ps, fill = "darkgrey"), color = "white", alpha = 0.6, position = 'identity', show.legend = T) +
geom_histogram(aes(x = ps, fill = as.factor(D)), colour = "white", alpha = 0.6, position = "identity") +
scale_fill_manual(values = c("#69b3a2", "#404080", "darkgrey"),
labels = c("untreated", "treated", "cumulative"), name = "Treatment Status")+
labs(x = "Propensity Score", y = "Count") +
theme_hc() +
theme(text = element_text(family = "Times New Roman"),
axis.title = element_text(size = 18),
axis.text = element_text(size = 14),
legend.title = element_text(size = 18),
legend.text = element_text(size = 14))
library(ggthemes)
# plot to check overlap between treated and untreated (Positivity Check)
ggplot(df_ps) +
geom_histogram(aes(x = ps, fill = "darkgrey"), color = "white", alpha = 0.6, position = 'identity', show.legend = T) +
geom_histogram(aes(x = ps, fill = as.factor(D)), colour = "white", alpha = 0.6, position = "identity") +
scale_fill_manual(values = c("#69b3a2", "#404080", "darkgrey"),
labels = c("untreated", "treated", "cumulative"), name = "Treatment Status")+
labs(x = "Propensity Score", y = "Count") +
theme_hc() +
theme(text = element_text(family = "Times New Roman"),
axis.title = element_text(size = 18),
axis.text = element_text(size = 14),
legend.title = element_text(size = 18),
legend.text = element_text(size = 14))
estimate_conditional_mean = function(Y, S, df_mean_reg, K = 5){
# create folds for cross-fitting
set.seed(777)
folds = createFolds(Y, k = K)
# create storage for estimations m_d_s
m11_hat = m10_hat = m01_hat = m00_hat = rep(0, length(Y))
# initiate the for loop over the folds
for (i in 1:K){
# split data in training and testing set
train_indices = unlist(folds[-i])
test_indices = unlist(folds[i])
# main regression for the prediction
mean_reg = cv.glmnet(as.matrix(df_mean_reg[train_indices, ]), Y[train_indices], family = "gaussian")
lambda = mean_reg$lambda.min
# m11_hat: test data (assuming D = 1, S = 1) and prediction
df_m11 = cbind(1, 1, df_mean_reg[,-c(1,2)])
m11_hat[test_indices] = predict(mean_reg, s = lambda,  newx = as.matrix(df_m11)[test_indices, ], type = "response")
# m01_hat: test data (assuming D = 0, S = 1) and prediction
df_m01 = cbind(0, 1, df_mean_reg[,-c(1,2)])
m01_hat[test_indices] = predict(mean_reg, s = lambda, newx = as.matrix(df_m01)[test_indices, ], type = "response")
# m10_hat: test data (assuming D = 1, S = 1) and prediction
df_m10 = cbind(1, 0, df_mean_reg[,-c(1,2)])
m10_hat[test_indices] = predict(mean_reg, s = lambda, newx = as.matrix(df_m10)[test_indices, ], type = "response")
# m00_hat: test data (assuming D = 0, S = 0) and prediction
df_m00 = cbind(0, 0, df_mean_reg[,-c(1,2)])
m00_hat[test_indices] = predict(mean_reg, s = lambda, newx = as.matrix(df_m00)[test_indices, ], type = "response")
}
# compute the conditional means of treated and untreated individuals
m1 = S*m11_hat + (1-S)*m10_hat
m0 = S*m01_hat + (1-S)*m00_hat
return(list(m1 = m1, m0 = m0, m11_hat = m11_hat, m01_hat = m01_hat, m10_hat = m10_hat, m00_hat = m00_hat))
}
# prepare the df for the conditional mean estimation
df_mean_reg = data %>% select(D, S, everything(), -Y, -C2, -Z)
# estimate the conditional means
elements = estimate_conditional_mean(Y = data$Y, S = data$S, df_mean_reg = df_mean_reg, K = 5)
#### Prepare final dataset to be used for algorithms
df = data %>% select(Y, D, S, C3, paste0("X", c(1:5))) %>%
mutate(X1_D = ifelse(X1 > mean(X1), 1, 0),
X2_D = ifelse(X2 > mean(X2), 1, 0),
X3_D = ifelse(X3 > mean(X3), 1, 0),
X4_D = ifelse(X4 > mean(X4), 1, 0),
X5_D = ifelse(X5 > mean(X5), 1, 0),
ps = df_ps$ps, m1 = elements$m1, m0 = elements$m0,
m11_hat = elements$m11_hat,
m10_hat = elements$m10_hat,
m01_hat = elements$m01_hat,
m00_hat = elements$m00_hat)
# take a random sample of n = 500 observations for the optimisation
set.seed(000)
df_sample = slice_sample(.data = df, n = 500)
# comparison between sample and full dataset for important variables
df_sample_comparison = df_sample %>% select(D, S, ps, m1, m0)
data_comparison = data %>% select(D, S) %>%
mutate(ps = df_ps$ps, m1 = elements$m1, m0 = elements$m0)
optimiseRegularised = function(Y, X, D, S, ps, m1, m0, alpha = mean(1-S), tolerance = 1e-6,
capacity_constraint, timelimit = 5000, lambda = 0.004){
# load libraries
library(Matrix)
library(slam)
library(gurobi)
# compute the doubly robust score (drs)
G11_hat = (S/mean(S)) * ((D/ps)*(Y-m1)+m1) # treated females
G01_hat = (S/mean(S)) * (((1-D)/(1-ps))*(Y-m0)+m0) # untreated females
G10_hat = ((1-S)/mean(1-S)) * ((D/ps)*(Y-m1)+m1) # treated males
G00_hat = ((1-S)/mean(1-S)) * (((1-D)/(1-ps))*(Y-m0)+m0) # untreated males
GS1 = G11_hat - G01_hat # female
GS0 = G10_hat - G00_hat # male
G = alpha*GS0 + (1-alpha)*GS1 # scaled vector
# add intercept for beta0, and normalise
X = as.matrix(cbind(1, X))
max_val = max(apply(X, 1, function(x) max(abs(x))))
XX = X/max_val
# set parameters
n = nrow(X)
p = ncol(X)
## initialise the model
model = list()
# sense of the optimisation, maximise utility
model$modelsense = "max"
# objective function contains the welfare & regularisation components
model$obj = c(G, rep(0, p), rep(-lambda, 2))
# set the linear constraint matrix
model$A = rbind(cbind(diag(1, nrow = n), -X, 0, 0), # policies - betas <= 1
cbind(diag(1, nrow = n), -X, 0, 0), # policies - betas > 0
c(rep(1, n), rep(0, p+2)), # treated <= capacity constraint
c(-((1-S)/mean(1-S) - S/mean(S)), rep(0, p), -1, 0), # -w - policies <= 0 (male - female)
c(((1-S)/mean(1-S) - S/mean(S)), rep(0, p), 0, -1)) # policies - w <= 0 (male - female)
# the rhs of the constraints, with tolerance (1e-6)
model$rhs = c(rep(1 - tolerance, n), rep(tolerance, n), capacity_constraint, rep(0, 2))
# set the constraint directions
model$sense = c(rep("<=", n), rep(">", n), rep("<=", 3))
# define the type of the variables
model$vtype = c(rep("B", n), rep("C", p+2))
# Put bounds on the parameter space, for z [0,1] and for the betas [-1,1], Inf for the slack variables
model$ub = c(rep(1, n+p), rep(Inf, 2))
model$lb = c(rep(0, n), rep(-1, p), rep(0, 2))
# set additional parameters for the optimisation
params = list(IntFeasTol = 1e-9, FeasibilityTol = 1e-9, TimeLimit = timelimit, # tolerance limits
BarConvTol = exp(-2), # tolerance on the barrier between primal and dual solution
Disconnected = 0, # degree of exploitation of (independent) submodels
Heuristics = 0, # fraction of runtime spent on feasibility heuristics
NodefileStart = 0.5) # max node memory before writing to hard drive
# solve the model
result = gurobi(model, params = params)
# extract the values
beta = result$x[(n+1):(n+p)]
policies = apply(X, 1, function(x) ifelse(x%*%beta > 0, 1, 0))
objval = result$objval
return(list(beta = beta, policies = policies, objval = objval, lambda = lambda,
result.x = result$x, time = result$runtime, gap = result$mipgap))
}
library(slam)
library(Matrix)
library(gurobi)
library(dplyr)
#### Preliminaries
load("./results/implementation.RData")
Y = df_sample$Y
D = df_sample$D
S = df_sample$S
m1 = df_sample$m1
m0 = df_sample$m0
ps = df_sample$ps
X = df_sample %>% select(C3, X1_D, X2_D, X3_D, X4_D, X5_D)
capacity_constraint = floor(0.33*nrow(X))
# regularisation with lambda = 0.004
FPT_regularised_004 = optimiseRegularised(Y, X = cbind(S,X), D, S, ps, m1, m0, alpha = mean(1-S), tolerance = 1e-6,
capacity_constraint, timelimit = 150, lambda = 0.004)
library(caret)
library(mltools)
optimiseRegularised = function(Y, X, D, S, ps, m1, m0, alpha = mean(1-S), tolerance = 1e-6,
capacity_constraint, timelimit = 5000, lambda = 0.004){
# load libraries
library(Matrix)
library(slam)
library(gurobi)
# compute the doubly robust score (drs)
G11_hat = (S/mean(S)) * ((D/ps)*(Y-m1)+m1) # treated females
G01_hat = (S/mean(S)) * (((1-D)/(1-ps))*(Y-m0)+m0) # untreated females
G10_hat = ((1-S)/mean(1-S)) * ((D/ps)*(Y-m1)+m1) # treated males
G00_hat = ((1-S)/mean(1-S)) * (((1-D)/(1-ps))*(Y-m0)+m0) # untreated males
GS1 = G11_hat - G01_hat # female
GS0 = G10_hat - G00_hat # male
G = alpha*GS0 + (1-alpha)*GS1 # scaled vector
# add intercept for beta0, and normalise
X = as.matrix(cbind(1, X))
max_val = max(apply(X, 1, function(x) max(abs(x))))
XX = X/max_val
# set parameters
n = nrow(X)
p = ncol(X)
## initialise the model
model = list()
# sense of the optimisation, maximise utility
model$modelsense = "max"
# objective function contains the welfare & regularisation components
model$obj = c(G, rep(0, p), rep(-lambda, 2))
# set the linear constraint matrix
model$A = rbind(cbind(diag(1, nrow = n), -X, 0, 0), # policies - betas <= 1
cbind(diag(1, nrow = n), -X, 0, 0), # policies - betas > 0
c(rep(1, n), rep(0, p+2)), # treated <= capacity constraint
c(-((1-S)/mean(1-S) - S/mean(S)), rep(0, p), -1, 0), # -w - policies <= 0 (male - female)
c(((1-S)/mean(1-S) - S/mean(S)), rep(0, p), 0, -1)) # policies - w <= 0 (male - female)
# the rhs of the constraints, with tolerance (1e-6)
model$rhs = c(rep(1 - tolerance, n), rep(tolerance, n), capacity_constraint, rep(0, 2))
# set the constraint directions
model$sense = c(rep("<=", n), rep(">", n), rep("<=", 3))
# define the type of the variables
model$vtype = c(rep("B", n), rep("C", p+2))
# Put bounds on the parameter space, for z [0,1] and for the betas [-1,1], Inf for the slack variables
model$ub = c(rep(1, n+p), rep(Inf, 2))
model$lb = c(rep(0, n), rep(-1, p), rep(0, 2))
# set additional parameters for the optimisation
params = list(IntFeasTol = 1e-9, FeasibilityTol = 1e-9, TimeLimit = timelimit, # tolerance limits
BarConvTol = exp(-2), # tolerance on the barrier between primal and dual solution
Disconnected = 0, # degree of exploitation of (independent) submodels
Heuristics = 0, # fraction of runtime spent on feasibility heuristics
NodefileStart = 0.5) # max node memory before writing to hard drive
# solve the model
result = gurobi(model, params = params)
# extract the values
beta = result$x[(n+1):(n+p)]
policies = apply(X, 1, function(x) ifelse(x%*%beta > 0, 1, 0))
objval = result$objval
return(list(beta = beta, policies = policies, objval = objval, lambda = lambda,
result.x = result$x, time = result$runtime, gap = result$mipgap))
}
library(slam)
library(Matrix)
library(gurobi)
library(dplyr)
#### Preliminaries
load("./results/implementation.RData")
Y = df_sample$Y
D = df_sample$D
S = df_sample$S
m1 = df_sample$m1
m0 = df_sample$m0
ps = df_sample$ps
X = df_sample %>% select(C3, X1_D, X2_D, X3_D, X4_D, X5_D)
capacity_constraint = floor(0.33*nrow(X))
# regularisation with lambda = 0.004
FPT_regularised_004 = optimiseRegularised(Y, X = cbind(S,X), D, S, ps, m1, m0, alpha = mean(1-S), tolerance = 1e-6,
capacity_constraint, timelimit = 15000, lambda = 0.004)
?cv.glmnet
library(glmnet)
?cv.glmnet()
